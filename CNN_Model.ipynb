{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29e651e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, mean_absolute_error, r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Conv1D, MaxPooling1D, Flatten, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9e724b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('preprocessed_data/preprocessed_LSTM_CNN.pkl', 'rb') as f:\n",
    "    LSTM_CNN_preprocessed_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30ba9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_CNN_preprocessed_data[4][8][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92669ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The variable LSTM_CNN_preprocessed_data is a list of our 5 bins [0-4][][]. Each bin contains the 15 belonging stocks or ETFs [0-4][0-14][].\n",
    "# After accessing the stock we can get the we are left with a 3 element list containing [0] X the features, [1] y the target and [2] the ticker symbol [0-4][0-14][0-2].\n",
    "\n",
    "# For the NNs the accessed data is 3D (samples, sample_size, features_per_sample)\n",
    "\n",
    "LSTM_CNN_preprocessed_data[2][7][0].shape\n",
    "# This for example accesses the X data from the 8th stock in bin 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1073d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we pull out the model creation process for clarity\n",
    "\n",
    "def build_cnn_model(params, X_train):\n",
    "    # create the model\n",
    "    model = Sequential()\n",
    "    for k in range(params['n_layers']):\n",
    "        if (k == 0): \n",
    "            model.add(Conv1D(filters=params['n_filters'], padding=\"same\",kernel_size=params['kernel_size'], activation=\"relu\",\n",
    "                        input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "        else:\n",
    "            model.add(Conv1D(filters=params['n_filters'], padding=\"same\",kernel_size=params['kernel_size'], activation=\"relu\"))\n",
    "        model.add(MaxPooling1D(pool_size=params['pool_size']))\n",
    "        model.add(Dropout(rate=params['dropout_rate']))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units=params['dense_units'], activation=\"relu\"))\n",
    "    model.add(Dense(1, activation=\"linear\"))\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=params['learning_rate'])\n",
    "    model.compile(optimizer=optimizer, loss=\"mse\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b054a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only pass pandas DataFrames as data_X data_y to the function below.\n",
    "# create windows \n",
    "def create_windows(data_X, data_y, window):\n",
    "    X, y, indices = [], [], []\n",
    "    for i in range(window, len(data_X)):\n",
    "        X.append(data_X[i - window:i].values.astype(\"float32\"))\n",
    "        y.append(float(data_y.iloc[i-1])) # i-1 because we shifted the target by one in the General preprocessing pipeline \n",
    "        indices.append(data_y.index[i-1])\n",
    "    return np.array(X, dtype=\"float32\"), np.array(y, dtype=\"float32\"), np.array(indices)\n",
    "\n",
    "#Create our CNN time series objective with data sets\n",
    "def creat_cnn_objective(X, y):\n",
    "    def cnn_objective(trial):\n",
    "        # Suggest hyperparameters\n",
    "        n_filters = trial.suggest_int(\"n_filters\", 16, 128, step=16)\n",
    "        n_layers = trial.suggest_int(\"n_layers\", 1, 2)\n",
    "        kernel_size = trial.suggest_int(\"kernel_size\", 2, 8)\n",
    "        pool_size = trial.suggest_int(\"pool_size\", 2, 4)\n",
    "        dense_units = trial.suggest_int(\"dense_units\", 16, 128, step=16)\n",
    "        dropout_rate = trial.suggest_float(\"dropout_rate\", 0.0, 0.5, step=0.1)\n",
    "        learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 1e-2, log=True)\n",
    "        batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n",
    "        min_window_size = ((pool_size + (kernel_size-1)) * pool_size) + (kernel_size-1) + 10 #calculate min window size based on other hyperparameters and add a margin of 10\n",
    "        window_size = trial.suggest_int(\"window_size\", min_window_size, 110, step=10)\n",
    "\n",
    "\n",
    "        # Start create windows and split the data\n",
    "        # training 0.9, validation 0.1, test 0.1\n",
    "        X_windows, y_windows, indices = create_windows(X, y, window_size)\n",
    "\n",
    "        end_train_set = X.index[int(X.shape[0] * 0.8)]\n",
    "        end_validation_set = X.index[int(X.shape[0] * 0.9)]\n",
    "\n",
    "        train_mask = indices < end_train_set\n",
    "        validation_mask = (indices >= end_train_set) & (indices < end_validation_set)\n",
    "        test_mask = indices >= end_validation_set\n",
    "\n",
    "        X_train = X_windows[train_mask]\n",
    "        y_train = y_windows[train_mask]\n",
    "        X_val = X_windows[validation_mask]\n",
    "        y_val = y_windows[validation_mask]\n",
    "        X_test = X_windows[test_mask]\n",
    "        y_test = y_windows[test_mask]\n",
    "        # End create windows\n",
    "\n",
    "        # parameters for the model to pass further to the build_cnn_model function\n",
    "        params = {\n",
    "            \"n_filters\": n_filters,\n",
    "            \"n_layers\": n_layers,\n",
    "            \"kernel_size\": kernel_size,\n",
    "            \"pool_size\": pool_size,\n",
    "            \"dense_units\": dense_units,\n",
    "            \"dropout_rate\": dropout_rate,\n",
    "            \"learning_rate\": learning_rate\n",
    "        }\n",
    "\n",
    "        # create the model\n",
    "        model = build_cnn_model(params, X_train)\n",
    "\n",
    "        early_stop = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
    "\n",
    "        # Train\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=50,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=[early_stop],\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        # compute errors\n",
    "        y_pred = model.predict(X_val, batch_size=batch_size)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "        mae = mean_absolute_error(y_val, y_pred)\n",
    "        r2 = r2_score(y_val, y_pred)\n",
    "        mape = np.mean(np.abs((y_pred - y_val) / y_val)) * 100\n",
    "        nrmse = np.sqrt(mean_squared_error(y_val, y_pred)) / np.mean(y_val) * 100  # in percent\n",
    "\n",
    "        # store the results in the trial\n",
    "        trial.set_user_attr(\"mae\", mae)\n",
    "        trial.set_user_attr(\"r2\", r2)\n",
    "        trial.set_user_attr(\"rmse\", rmse)\n",
    "        trial.set_user_attr(\"nrmse\", nrmse)\n",
    "        \n",
    "        return mape\n",
    "    return cnn_objective\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6111c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of trials per asset\n",
    "N_trials = 70\n",
    "\n",
    "# outer list to store results for each bin\n",
    "CNN_data_results = []\n",
    "\n",
    "for i in range(len(LSTM_CNN_preprocessed_data)):\n",
    "    \n",
    "    # inner list to store results for each asset\n",
    "    CNN_bin_results = []\n",
    "    for j in range(len(LSTM_CNN_preprocessed_data[i])):\n",
    "        X, y, symbol = LSTM_CNN_preprocessed_data[i][j]\n",
    "\n",
    "\n",
    "        # 5.2. CNN study\n",
    "        study = optuna.create_study(\n",
    "            direction=\"minimize\",\n",
    "            study_name=\"cnn_regression_study\",\n",
    "            sampler=optuna.samplers.TPESampler(),\n",
    "            pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=10)\n",
    "        )\n",
    "        study.optimize(creat_cnn_objective(X, y), n_trials=N_trials)\n",
    "\n",
    "        params = study.best_params\n",
    "\n",
    "        # create windows and split the data\n",
    "        # training 0.8, validation 0.1, test 0.1\n",
    "        X_windows, y_windows, indices = create_windows(X, y, params['window_size'])\n",
    "\n",
    "        end_train_set = X.index[int(X.shape[0] * 0.8)]\n",
    "        end_validation_set = X.index[int(X.shape[0] * 0.9)]\n",
    "\n",
    "        train_mask = indices < end_train_set\n",
    "        validation_mask = (indices >= end_train_set) & (indices < end_validation_set)\n",
    "        test_mask = indices >= end_validation_set\n",
    "\n",
    "        X_train = X_windows[train_mask]\n",
    "        y_train = y_windows[train_mask]\n",
    "        X_val = X_windows[validation_mask]\n",
    "        y_val = y_windows[validation_mask]\n",
    "        X_test = X_windows[test_mask]\n",
    "        y_test = y_windows[test_mask]\n",
    "\n",
    "        # create the model\n",
    "        model = build_cnn_model(params, X_train)\n",
    "\n",
    "        # Train\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=50,\n",
    "            batch_size=params['batch_size'],\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        # compute errors\n",
    "        y_pred = model.predict(X_test, batch_size=params['batch_size'])\n",
    "        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        test_mae = mean_absolute_error(y_test, y_pred)\n",
    "        test_r2 = r2_score(y_test, y_pred)\n",
    "        test_mape = np.mean(np.abs((y_pred - y_test) / y_test)) * 100\n",
    "        test_nrmse = np.sqrt(mean_squared_error(y_test, y_pred)) / np.mean(y_test) * 100  # in percent\n",
    "\n",
    "        # store the results in the trial\n",
    "        results = {\n",
    "            \"test_mape\": test_mape,\n",
    "            \"test_rmse\": test_rmse,\n",
    "            \"test_mae\":  test_mae,\n",
    "            \"test_r2\":   test_r2\n",
    "        }\n",
    "\n",
    "\n",
    "        CNN_bin_results.append(results)\n",
    "    # write out the results for each bin\n",
    "    with open(f'results_ret_log/CNN/Performance_metrices_CNN_bin_{i}.pkl', 'wb') as f:\n",
    "        pickle.dump(CNN_bin_results, f)\n",
    "    CNN_data_results.append(CNN_bin_results)\n",
    "# write out full results\n",
    "with open(f'results_ret_log/CNN/Performance_metrices_CNN_full.pkl', 'wb') as f:\n",
    "    pickle.dump(CNN_data_results, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
