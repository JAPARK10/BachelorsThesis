{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b13254d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, mean_absolute_error, r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Conv1D, MaxPooling1D, Flatten, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24010034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load preproessed data \n",
    "with open('preprocessed_data/preprocessed_LSTM_CNN.pkl', 'rb') as f:\n",
    "    LSTM_CNN_preprocessed_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf2cd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The variable LSTM_CNN_preprocessed_data is a list of our 5 bins [0-4][][]. Each bin contains the 15 belonging stocks or ETFs [0-4][0-14][].\n",
    "# After accessing the stock we can get the we are left with a 2 element list containing [0] X the features and [1] y the target [0-4][0-14][0-1].\n",
    "\n",
    "# For the NNs the accessed data is 3D (samples, sample_size, features_per_sample)\n",
    "\n",
    "LSTM_CNN_preprocessed_data[2][7][0].shape\n",
    "# This for example accesses the X data from the 8th stock in bin 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da0af55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only pass pandas DataFrames as data_X data_y to the function below.\n",
    "def create_windows(data_X, data_y, window):\n",
    "    X, y, indices = [], [], []\n",
    "    for i in range(window, len(data_X)):\n",
    "        X.append(data_X[i - window:i].values.astype(\"float32\"))\n",
    "        y.append(float(data_y.iloc[i-1])) # i-1 because we shifted the target by one in the General preprocessing pipeline \n",
    "        indices.append(data_y.index[i-1])\n",
    "    return np.array(X, dtype=\"float32\"), np.array(y, dtype=\"float32\"), np.array(indices)\n",
    "\n",
    "# Create the tuning objective with according data sets\n",
    "def make_lstm_objective(X, y):\n",
    "    def lstm_objective(trial):\n",
    "        # Suggest hyperparameters\n",
    "        n_units = trial.suggest_int(\"lstm_units\", 32, 128, step=32)\n",
    "        n_layers = trial.suggest_int(\"lstm_layers\", 1, 2, step=1)\n",
    "        dropout_rate = trial.suggest_float(\"dropout_rate\", 0.0, 0.5, step=0.1)\n",
    "        learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 1e-2, log=True)\n",
    "        batch_size = trial.suggest_categorical(\"batch_size\", [16, 32])\n",
    "        n_dense = trial.suggest_int(\"dense_units\", 16, 128, step=16)\n",
    "        window_size = trial.suggest_int(\"window_size\", 10, 60, step=10)\n",
    "\n",
    "        # create windows\n",
    "        X_windows, y_windows, indices = create_windows(X, y, window_size)\n",
    "\n",
    "        # split data in train 0.8, validation 0.1 and test 0.1\n",
    "        end_train_set = X.index[int(X.shape[0] * 0.8)]\n",
    "        end_validation_set = X.index[int(X.shape[0] * 0.9)]\n",
    "\n",
    "        train_mask = indices < end_train_set\n",
    "        validation_mask = (indices >= end_train_set) & (indices < end_validation_set)\n",
    "        test_mask = indices >= end_validation_set\n",
    "\n",
    "        X_train = X_windows[train_mask]\n",
    "        y_train = y_windows[train_mask]\n",
    "        X_val = X_windows[validation_mask]\n",
    "        y_val = y_windows[validation_mask]\n",
    "        X_test = X_windows[test_mask]\n",
    "        y_test = y_windows[test_mask]\n",
    "        \n",
    "\n",
    "        # Build model\n",
    "        model = Sequential()\n",
    "        for i in range(n_layers):\n",
    "            bool_return_sequences = (i < n_layers - 1)  # Return sequences for all but the last layer\n",
    "            bool_firtst_layer = (i == 0)  # First layer needs input shape\n",
    "            if bool_firtst_layer:\n",
    "                model.add(LSTM(units=n_units, input_shape=(window_size, X_train.shape[2]),\n",
    "                            return_sequences=bool_return_sequences))\n",
    "            else:\n",
    "                model.add(LSTM(units=n_units, return_sequences=bool_return_sequences))\n",
    "            model.add(Dropout(rate=dropout_rate))\n",
    "        model.add(Dense(1, activation=\"linear\"))  # regression\n",
    "\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss=\"mse\")\n",
    "\n",
    "        # Early stopping callback to prune unpromising trials\n",
    "        early_stop = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
    "\n",
    "        # Train\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=50,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=[early_stop],\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        # evaluate and calculate errors\n",
    "        y_pred = model.predict(X_val, batch_size=batch_size)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "        mae = mean_absolute_error(y_val, y_pred)\n",
    "        r2 = r2_score(y_val, y_pred)\n",
    "        mape = np.mean(np.abs((y_pred - y_val) / y_val)) * 100\n",
    "        nrmse = np.sqrt(mean_squared_error(y_val, y_pred)) / np.mean(y_val) * 100  # in percent\n",
    "\n",
    "        # Store results in trial\n",
    "        trial.set_user_attr(\"mae\", mae)\n",
    "        trial.set_user_attr(\"r2\", r2)\n",
    "        trial.set_user_attr(\"rmse\", rmse)\n",
    "        trial.set_user_attr(\"nrmse\", nrmse)\n",
    "\n",
    "        return mape\n",
    "    return lstm_objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f054db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the study\n",
    "\n",
    "# outer list to store results for each bin\n",
    "LSTM_study_results = []\n",
    "\n",
    "N_TRIALS = 70\n",
    "for i in range(0, len(LSTM_CNN_preprocessed_data)):\n",
    "    # list per bin to store results of single asset\n",
    "    LSTM_bin_results = []\n",
    "\n",
    "    for j in range(0, len(LSTM_CNN_preprocessed_data[i])):\n",
    "\n",
    "        print(f\"Running LSTM study for bin {i+1}, stock {j+1}...\")\n",
    "        X, y, symbol = LSTM_CNN_preprocessed_data[i][j]\n",
    "\n",
    "        # create LSTM study\n",
    "        lstm_objective = make_lstm_objective(X, y)\n",
    "        lstm_study = optuna.create_study(\n",
    "            direction=\"minimize\",\n",
    "            study_name=\"lstm_regression_study\",\n",
    "            sampler=optuna.samplers.TPESampler(),\n",
    "            pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=10)\n",
    "        )\n",
    "        lstm_study.optimize(lstm_objective, n_trials=N_TRIALS)\n",
    "\n",
    "        best_params = lstm_study.best_trial.params\n",
    "        window_size = best_params[\"window_size\"]\n",
    "        \n",
    "        #create windows\n",
    "        # split data in train 0.9 and test 0.1\n",
    "        X_windows, y_windows, indices = create_windows(X, y, window_size)\n",
    "\n",
    "        end_train_validation_set = X.index[int(X.shape[0] * 0.9)]\n",
    "\n",
    "        train_mask = indices < end_train_validation_set\n",
    "        test_mask = indices >= end_train_validation_set\n",
    "\n",
    "        X_train_val = X_windows[train_mask]\n",
    "        y_train_val = y_windows[train_mask]\n",
    "        X_test = X_windows[test_mask]\n",
    "        y_test = y_windows[test_mask]\n",
    "\n",
    "        #Build the model\n",
    "        model = Sequential()\n",
    "        for k in range(best_params[\"lstm_layers\"]):\n",
    "            return_seq = (k < best_params[\"lstm_layers\"] - 1)\n",
    "            if k ==0:\n",
    "                model.add(LSTM( best_params[\"lstm_units\"],\n",
    "                            input_shape=(window_size, X_train_val.shape[2]),\n",
    "                            return_sequences=return_seq) )\n",
    "            else:\n",
    "                model.add( LSTM(best_params[\"lstm_units\"], return_sequences=return_seq))\n",
    "            model.add(Dropout(best_params[\"dropout_rate\"]))\n",
    "        model.add(Dense(1, activation= \"linear\"))\n",
    "\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=best_params[\"learning_rate\"])\n",
    "        model.compile(optimizer=optimizer, loss=\"mean_absolute_percentage_error\")\n",
    "\n",
    "        # Train the model based on the best parameters for the particular asset\n",
    "        model.fit(\n",
    "            X_train_val, y_train_val,\n",
    "            epochs=50,\n",
    "            batch_size=best_params['batch_size'],\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        # calculate errors\n",
    "        y_pred = model.predict(X_test, batch_size=best_params[\"batch_size\"])\n",
    "        test_mape   = np.mean(np.abs((y_pred - y_test) / y_test)) * 100\n",
    "        test_rmse   = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        test_mae    = mean_absolute_error(y_test, y_pred)\n",
    "        test_r2     = r2_score(y_test, y_pred)\n",
    "\n",
    "        results = {\n",
    "            \"test_mape\": test_mape,\n",
    "            \"test_rmse\": test_rmse,\n",
    "            \"test_mae\":  test_mae,\n",
    "            \"test_r2\":   test_r2\n",
    "        }\n",
    "\n",
    "\n",
    "        LSTM_bin_results.append(results)\n",
    "        # write out the results\n",
    "        with open(f'results/LSTM/bin{i+1}/Performance_metrices_LSTM_bin_{i+1}_stock_{j+1}.pkl', 'wb') as f:\n",
    "            pickle.dump(results, f)\n",
    "    LSTM_study_results.append(LSTM_bin_results)\n",
    "    # write out the results\n",
    "    with open(f'results/LSTM/Performance_metrices_LSTM_bin_{i+1}.pkl', 'wb') as f:\n",
    "        pickle.dump(LSTM_bin_results, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
